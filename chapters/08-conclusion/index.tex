\chapter{Conclusion}
\label{cha:conclusion}

The main interest for static site generators evolved during my work at a digital performance monitoring company based in Linz, Upper Austria. I was impressed by the simplicity of generating HTML content without having to construct an extensive interface before even getting to the point of actually creating content. One of the major drawbacks although was the idle time I had to face during a rendering cycle.

One of the projects I used to work with was initially based on Jekyll, but with some strong customizations added to the build pipeline setup. Consisting of a reasonable amount of content files, a build cycle sometimes lasted more than 20 minutes -- mostly due to heavy tasks, such as picture resizing, etc.

Therefore, this project was originally designed only as supporting tool for local development, but it quickly grew out of hand as I figured out, that this type of development is facing too many limitations. One of the very first approaches was to use the GitHub API, as I had already gained some experience in using it while working on a few projects in the past. Concerning the amount of information needed, and first and foremost where to actually fetch it, GitHub is the best possible tool to use, unless a strictly local solution is preferred. Soon after, it was clear to build something, which is able to act remotely and as automated as possible.

However, the major premise for this project was to provide an unopinionated tool for rendering a website with a caching solution included. Although this may sound fairly understandable in the first place, it soon turned out, that this mixture is also going to be the biggest challenge in finding a suitable way of solving this problem statement.

As a conclusion, I can now say, the most interesting part about my research was not only to find ways to overcome those local performance issues during rebuilds, but also trying to leverage the common workflow in moving as many local tasks to a remote workspace as possible. This should support content authors and developers in focusing on their core jobs by taking unnecessary responsibilities off their hands.

Soon after my initial project setup, I was already forced to balance the importance of the core principles and therefore I had to compromise over some of them. First, the project is not as unopinionatedly usable as originally planned, as by all forms of customizability, Metalsmith needs at least a core structure of parameters in its configuration file. To not interfere with the standard configuration file, I designed an adjusted format, by also allowing it to be written in YAML. This especially should support developers switching from Jekyll.

Second, by providing an automated remote workspace, the question of long-term storage has to be reconsidered, as the tar.gz archive currently only gets written to the same file structure the REST API lies in. This saves time by always having the latest version at hand, however, using an external storage like Amazon S3\footnote{\url{https://aws.amazon.com/s3/?hp=tile&so-exp=below} -- Amazon S3 website.}, the file distribution would scale significantly better (especially when using multiple instances in Docker containers, etc\ldots) and is also a lot cheaper in the long run.

Third, the caching algorithm currently appears very basic, as the variety of future repositories cannot be correctly evaluated by now. Therefore it needs some kind of machine learning, which is able to virtualize a dependency graph throughout a single repository (see Sec. \ref{sec:chacheimprovement-machinelearning}). However, this would cover the extent of a project on its own.

Lastly it can be said, that the overall performance of the REST API handling different smaller demo projects during my tests was quite the same, as both a repository containing 24 files and a repository containing roughly over 300 files needed little over 30 seconds for an initial build. A much more interesting examination would be testing the REST API in a productive ecosystem, as the ``real'' needs of a comparable development team would be revealed much faster and much more precise. Depending on these informations, future development could be led towards fields which really matter.

To sum everything up, the outcome is quite the initially expected extent; a proof of concept, which is able to produce a usable website on the one hand, but on the other hand should as well demonstrate the difficulties of providing and running an unopinionated, semi-automated system, which should be able to work with highly diverse source repositories together with as many requests as possible at the same time. This project shows that all of this is possible, if the end user is able to compromise on his expectations and development is pushed further towards a more user-friendly surrounding.
%% verschiedenheit der projekte, unopinionated, begrenztheit der vielfalt, etc..
